{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import spacy\n",
    "\n",
    "import datetime\n",
    "from time import time\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf. __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AMM_A350.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "#sentences = data['PARAfull'][:20]\n",
    "#data\n",
    "# 1226552 rows\n",
    "\n",
    "data = data[data['POStxt'].notna()]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Count\"]=data.groupby(\"POStxt\")[\"POStxt\"].transform('count')\n",
    "data_reduced = data.drop_duplicates(['POStxt'])\n",
    "sentences = list(data_reduced['POStxt'])\n",
    "count = list(data_reduced['Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_with_eos_and_unk.pkl', 'rb') as f:\n",
    "    sentences, counter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(counter.keys())\n",
    "vocab_size = len(vocab)\n",
    "num_of_words = sum(counter.values())\n",
    "print('There are %d sentences in our dataset.' % len(sentences))\n",
    "print('There are %d total words in our dataset.' % num_of_words)\n",
    "print('There are %d unique words in our dataset.' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data: keep stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_keep_stopwords(doc):\n",
    "    # Lemmatizes but I don't remove stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider correcting the preprocessing to include <eos> and <unk>. I may use https://spacy.io/api/language#add_pipe, or \n",
    "# I can re run the sentences without the '<' and '>'.\n",
    "# I want to keep <eos> and <unk>, so I don't use the breaf cleaning below by now. Moreover,\n",
    "# the n_threads is no longer used in nlp pipe\n",
    "#brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in sentences)\n",
    "#txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "t = time()\n",
    "\n",
    "brief_cleaning = (str(row).lower() for row in sentences)\n",
    "txt_keep_stopwords = [cleaning_keep_stopwords(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_split_keep_stopwords = [row.split() for row in txt_keep_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(txt_split[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "#logging.debug(\"test\")\n",
    "\n",
    "phrases_keep_stopwords = Phrases(txt_split_keep_stopwords, min_count=30, progress_per=10000)\n",
    "print(phrases_keep_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_keep_stopwords = Phraser(phrases_keep_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_keep_stopwords_final = bigram_keep_stopwords[txt_split_keep_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_keep_stopwords_final[0])\n",
    "print(txt_keep_stopwords_final[1])\n",
    "print(txt_keep_stopwords_final[2])\n",
    "print(len(txt_keep_stopwords_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_keep_stopwords = defaultdict(int)\n",
    "for sent in txt_keep_stopwords_final:\n",
    "    for i in sent:\n",
    "        word_freq_keep_stopwords[i] += 1\n",
    "\n",
    "count_less_freq_words = 0\n",
    "\n",
    "for word in word_freq_keep_stopwords.keys():\n",
    "    if word_freq_keep_stopwords[word] < 5:\n",
    "        count_less_freq_words += 1\n",
    "        \n",
    "print('There are %d words in our dataset after preprocessing (keep stopwords, lemmatize and merge bigrams),' % len(word_freq_keep_stopwords)) \n",
    "print('and now there are %d words appearing less than 5 times.' % count_less_freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_remove_stopwords(doc):\n",
    "    # Lemmatizes but I don't remove stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider correcting the preprocessing to include <eos> and <unk>. I may use https://spacy.io/api/language#add_pipe, or \n",
    "# I can re run the sentences without the '<' and '>'.\n",
    "# I want to keep <eos> and <unk>, so I don't use the breaf cleaning below by now. Moreover,\n",
    "# the n_threads is no longer used in nlp pipe\n",
    "#brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in sentences)\n",
    "#txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "t = time()\n",
    "\n",
    "brief_cleaning = (str(row).lower() for row in sentences)\n",
    "txt_remove_stopwords = [cleaning_remove_stopwords(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_split_remove_stopwords = [row.split() for row in txt_remove_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_split_remove_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logger = logging.getLogger()\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "#logging.debug(\"test\")\n",
    "\n",
    "phrases_remove_stopwords = Phrases(txt_split_remove_stopwords, min_count=30, progress_per=10000)\n",
    "print(phrases_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_remove_stopwords = Phraser(phrases_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_remove_stopwords_final = bigram_remove_stopwords[txt_split_remove_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_remove_stopwords_final[0])\n",
    "print(txt_remove_stopwords_final[1])\n",
    "print(txt_remove_stopwords_final[2])\n",
    "print(len(txt_remove_stopwords_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_remove_stopwords = defaultdict(int)\n",
    "for sent in txt_remove_stopwords_final:\n",
    "    for i in sent:\n",
    "        word_freq_remove_stopwords[i] += 1\n",
    "\n",
    "count_less_freq_words = 0\n",
    "\n",
    "for word in word_freq_remove_stopwords.keys():\n",
    "    if word_freq_remove_stopwords[word] < 5:\n",
    "        count_less_freq_words += 1\n",
    "        \n",
    "print('There are %d words in our dataset after preprocessing (remove stopwords, lemmatize and merge bigrams).' % len(word_freq_remove_stopwords)) \n",
    "#print('and now there are %d words appearing less than 5 times.' % count_less_freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_remove_stop - set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare both vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keep_stop_words = {k: v for k, v in sorted(word_freq_keep_stopwords.items(), key=lambda item: item[1], reverse = True)}\n",
    "set_keep_stop = set(dict_keep_stop_words.keys())\n",
    "\n",
    "dict_remove_stop_words = {k: v for k, v in sorted(word_freq_remove_stopwords.items(), key=lambda item: item[1], reverse = True)}\n",
    "set_remove_stop = set(dict_remove_stop_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are %d words in vocab if we keep stop, and %d if we remove them.' % (len(set_keep_stop), len(set_remove_stop)) )\n",
    "print('In particular, %d words appear in the set WITH stopwords that disappear when we remove the stopwords' \n",
    "      % len(set_keep_stop - set_remove_stop) )\n",
    "print('However, %d new words appear in the set without stopwords. These are mostly bigrams obtained after removing the stopwords.' \n",
    "      % len(set_remove_stop - set_keep_stop) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_keep_stop - set_remove_stop\n",
    "\n",
    "# Without stop words there are many bigrams that do not appear in the set with stop words:\n",
    "#set_remove_stop - set_keep_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model: keep stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define model\n",
    "\n",
    "#loss_logger = LossLogger()\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "w2v_model_keep_stopwords = Word2Vec(min_count=5,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007,\n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build vocab\n",
    "\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model_keep_stopwords.build_vocab(txt_keep_stopwords_final, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d words in our dataset after building vocab with min_count = 5.' \n",
    "      % len(list(w2v_model_keep_stopwords.wv.key_to_index.keys()))) \n",
    "#print(w2v_model)\n",
    "#print(list(w2v_model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3) Train model\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model_keep_stopwords.train(txt_keep_stopwords_final, total_examples=w2v_model_keep_stopwords.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model: remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define model\n",
    "\n",
    "#loss_logger = LossLogger()\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "w2v_model_remove_stopwords = Word2Vec(min_count=5,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007,\n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build vocab\n",
    "\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model_remove_stopwords.build_vocab(txt_remove_stopwords_final, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d words in our dataset after building vocab with min_count = 5.' \n",
    "      % len(list(w2v_model_remove_stopwords.wv.key_to_index.keys()))) \n",
    "#print(w2v_model)\n",
    "#print(list(w2v_model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3) Train model\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model_remove_stopwords.train(txt_remove_stopwords_final, total_examples=w2v_model_remove_stopwords.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model_keep_stopwords.wv.most_similar(positive=[\"install\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_remove_stopwords.wv['install']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_remove_stopwords.wv.most_similar(positive=[\"install\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_model_keep_stopwords.wv.most_similar(positive=[\"press\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_remove_stopwords.wv.most_similar(positive=[\"press\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_keep_stopwords.wv.similarity(\"put\", 'tool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_remove_stopwords.wv.similarity(\"cabin\", 'cockpit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_keep_stopwords.wv.doesnt_match(['put', 'install', 'spacer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
